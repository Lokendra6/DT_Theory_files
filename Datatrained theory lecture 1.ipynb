{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Reading the data\n",
    "## import pandas as pd\n",
    "\n",
    "##df=pd.read_csv(\"datafile.csv\",header=None)\n",
    "##df.head()..top 5 rows\n",
    "##df.tail()..bottom 6 rows\n",
    "\n",
    "##assigning column name\n",
    "##headername=[\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "##df.columns=headername\n",
    "\n",
    "##to save file in csv format..df.to_csv(\"filename.csv\")\n",
    "\n",
    "##pd.read_csv(\"fielname.csv\").....df.to_csv(path)\n",
    "##pd.read_json(\"\")....df.to_json()\n",
    "##pd.read_sql(\" \")....df.to_sql()\n",
    "##pd.read_excel(\" \")...df.to_hdf()\n",
    "\n",
    "##First should check the columns n datatype after loading the data\n",
    "##Pandas Type int64,float64,datetime,obj {native python type..int,float,datetime,string}\n",
    "##Obj...numeric ans strings\n",
    "##int..numeric character\n",
    "##float..numeric character with decimal\n",
    "##datetime..timedate\n",
    "\n",
    "##check for type mismatch and potential info\n",
    "##compatibility with python methods\n",
    "\n",
    "##df.dtypes\n",
    "##df.describe()----statistical summary\n",
    "##for all column...df.describe(include=\"all\")\n",
    "##df.info()...for comncise summary of dataframe...show top 30 rows and bottom 30 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing Data/Data Cleaning/Data wrangling\n",
    "##Preprocessing..converting or mapping data from law form to another fomat in order to ready data for further analysis\n",
    "##Handling missing value\n",
    "##data formatting\n",
    "##Data normalization (centering /Scaling)\n",
    "##Data Binning\n",
    "##Converting categorical value to numeric varibales\n",
    "##access the column...df[\"columnname\"]\n",
    "##each of the column is a pandas series\n",
    "\n",
    "##Missing Value..\n",
    "##two way to handle..either drop missing value (drop the varibale or drop the data entry)\n",
    "## replace the missing value..replace by mean,frequency or any other function\n",
    "##Or leave missing data as it is\n",
    "\n",
    "##to drop value....df.dropna():\n",
    "##axis=0 for rows\n",
    "##axis=1 for columns\n",
    "##df.dropna(subset=[\"Price\"],axis=0,inplace=True)\n",
    "\n",
    "##to replace missing value\n",
    "##df.replace(0,df.mean())\n",
    "##df[\"losses\"].replace(np.nan,df[\"losses\"].mean())\n",
    "\n",
    "##Data Fromatting\n",
    "##Bring data into common standard of expression allow users to make meaningful comparison\n",
    "##suppose converting..df[\"mpg\"]=df[\"mpg\"]/100\n",
    "##df.rename(columns={\"mpg\":\"city-mpg\"},inplace=True)\n",
    "\n",
    "##to check datatype..df.datatypes()\n",
    "##to..change datatype..df.astype()\n",
    "##df[\"price\"]=df[\"price\"].astype(\"int\")\n",
    "\n",
    "##Data Normalization\n",
    "##normalization allow fair comparison between features making sure they have same impact\n",
    "##suppose income and age..so they have different range\n",
    "##in case of linear regression salary will influence more as comapred to age due to larger value\n",
    "##but both r important predictor...so nature data lead to biasness\n",
    "##Normalize both variables in the range zero to 1..\n",
    "##Normalization allow similar value range\n",
    "##similar intrinsic influence on analytical model\n",
    "## 3 technique..simple feature scaling..x(new)=x(old)/x(max)\n",
    "## Min-Max..x(new)=x(old)-x(min)/x(max)-x(min)\n",
    "## Zscore or standard score..x(old)-x(mean)/std...normally value range between..+-3\n",
    "\n",
    "## simple feature scaling..df[\"len\"]=df[\"len\"]/df[\"len\"].max()\n",
    "\n",
    "##min max method...df[\"len\"]=df[\"len\"]-df[\"len\"].min()/df[\"len\"].max()-df[\"len\"].min()\n",
    "\n",
    "##zscore method...df[\"len\"]=df[\"len\"]-df[\"len\"].mean()/df[\"len\"].std()\n",
    "\n",
    "##Binning in python..grouping of values into bin\n",
    "##sometimes binning improve accuracy of model\n",
    "##group a set of numerical values into a set of bins\n",
    "##convert numeric value in categorical variable\n",
    "##binwidth=int((max(df[\"price\"])-min(df[\"price\"]))/4)\n",
    "##bins=range(min(df[\"price\"]),max(df[\"price\"]),binwidth)\n",
    "##group=[\"Low\",\"Medium\",\"High\"]\n",
    "##df[\"price-binned\"]=pd.cut(df[\"price\"],bins,lebels=group)\n",
    "\n",
    "##Convert categorical variable into numerical variable\n",
    "##one hot encoding\n",
    "##add dummy variable for each unique category\n",
    "##assign 0 or 1 in each category\n",
    "##pd.get_dummies()...convert categorical variable to dummy variable(0,1)\n",
    "##ex. pd.get_dummies(df[\"fuel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EDA...\n",
    "##summarize main characterstic of data\n",
    "##gain bettre understanding of the dataset\n",
    "##uncover relationship between variable\n",
    "##extract important variables\n",
    "\n",
    "##dicuss about descriptive statistics ..GroupBy....ANOVA..Correlation...Advance correlation\n",
    "\n",
    "##Descriptive statistics...df.describe()\n",
    "##categorical variable..df[\"colour\"].value_counts()\n",
    "##variant=df[\"colour\"].value_counts()\n",
    "##variant.rename(columns={\"colour\":\"count\"},inplace=True)\n",
    "##variant.index.name=\"colour\"\n",
    "\n",
    "##box plot great way to visualize numeric data\n",
    "##sns.boxplot(x=\"age\",y=\"salary\",data=df)\n",
    "\n",
    "##Scatter plot..for relationship between two continous variable..predictor on xaxis..target on y axis\n",
    "##x=df[\"engine size\"]\n",
    "##y=df[\"price\"]\n",
    "##plt.scatter(x.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##GroupBy in Python\n",
    "##df.Groupby()\n",
    "##Can be applied on categorical variables\n",
    "##Grouop data into categories\n",
    "##Single or multiple varibales\n",
    "##df1=df[\"wheels\",\"bodystyle\",\"price\"]\n",
    "##df_grp=df1.groupby([\"wheels\",\"body-style\"],as_index=False).mean()\n",
    "##df_grp\n",
    "##pivot data\n",
    "##df_grp.pivot(index=\"wheels\",columns=\"body-style\")\n",
    "\n",
    "##ANOVA..Analysis of variance\n",
    "#@finding corelation between different groups of categorical variable\n",
    "##statistical comparison of groups\n",
    "##want to visulaize categorical variable and see relation between differet category\n",
    "##Ftestscore-variation between groups means divided by variation within the same group\n",
    "##p value=confidence degree\n",
    "##small F imply poor correlation between varibale category and target varibale\n",
    "##small F imply poor correlation between variable categories and target variable\n",
    "##high correlation if high F-test score value and small p value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correlation..Measures to what extend different vaiables are interdependent\n",
    "##Correlation does not imply caustion\n",
    "##sns.regplot(x=\"engine-size\",y=\"price\",data=df)\n",
    "##plt.ylim(0,)\n",
    "\n",
    "##Pearson correlation measure the strength of correlation between 2 features\n",
    "##measure correlation by using\n",
    "##correlation coefficent (-1 to 1) n P value (p<.05..give certainity of relation)\n",
    "##Strong correlation ..if correlation coefficient close to 1 or -1...n...p<.001\n",
    "##pearson_coef,p_value=stats,pearsonr[df[\"horsepower\"],df[\"power\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model Development\n",
    "##Model is an equation used to predict value given one or more other value\n",
    "##Relating one or more independent variable to dependent variable\n",
    "\n",
    "##Simple Linear Regression\n",
    "##Multiple Linear Regression\n",
    "##Polynomial Regression\n",
    "\n",
    "##simple linear has one independent variabale to make prediction\n",
    "##multiple linear regression has multiple independent variable to make a prediction\n",
    "\n",
    "##Regression plot..give direction..n strength of relatio ship\n",
    "##sns.regplot(x=\"mpg\",y=\"price\",data=df)\n",
    "##plt.ymin(0,)\n",
    "\n",
    "##Residual plot\n",
    "##sns.residplot(df[\"mpg\"],df[\"price\"])\n",
    "\n",
    "##distributionplot\n",
    "##ax1=sns.distplot(df[\"price\"],hist=False,color=\"r\",label=\"Actual Values\")\n",
    "##sns.distplot(Yhat,hist=False,color=\"b\",ax=ax1)\n",
    "\n",
    "##polynomial Regression\n",
    "##special case of general linear regression\n",
    "##useful for describing curvilinear relationships\n",
    "##By squaring or setting higher order terms of the predictor variable\n",
    "##square..2nd order polynomial regression\n",
    "##polyfit function\n",
    "##f=np.polyfit(x,y,3)...3rd order\n",
    "##p=np.polydl(f)\n",
    "##print(p)\n",
    "\n",
    "##for polynimail regression of more than one dimension\n",
    "##from sklearn.preprocessingimport PolynomialFeatures\n",
    "##pr=PolynomialFeatures(degree=2)\n",
    "##x_polly=pr.fit_transform(x[[\"horsepower\"]],[\"curbweight\"],include_bias=False)\n",
    "\n",
    "##As the dimension of data get larger,we may want to Normalize multiple feature in scikitlearn using StandardScaler\n",
    "##standardize each column\n",
    "\n",
    "##Pipelines\n",
    "##steps to getting prediction..normalization..then polynomialtransform...linear regression\n",
    "##to simply this steps we use pipelines\n",
    "##transformation and prediction\n",
    "\n",
    "##from sklearn.linear_model import LinearRegression\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "##from sklearn.preprocessing import PolynomialFeatures\n",
    "##from sklearn.pipeline import Pipeline\n",
    "\n",
    "##then we create list of tuple\n",
    "##input=[(\"scale\",StandardScaler()),(\"polynomial\",PolynomialFeatures(degree=2)),(\"mode\",LinearRegression())]\n",
    "##pipe=Pipeline(Input)\n",
    "##pipe is a pipeline object\n",
    "##pipe.train(x[\"horsepower\",\"curbweight\",\"enginesize\",\"mpg\"],y)\n",
    "##pred=pipe.predict(x[\"horsepower\",\"curbweight\",\"enginesize\",\"mpg\"])\n",
    "\n",
    "##Measures of Insample Evaluation\n",
    "##A way to numerically determine how good model fits on dataset\n",
    "##two important measures are..Mean Squred E(MSE)rror and R squared\n",
    "\n",
    "##Rsquared also called coefficient of determination\n",
    "##measure to determine how close data is to fitted regression line\n",
    "##R2 explains the percentage of variation of target variable that is explained by the linear model\n",
    "##R2 mostly take value betwwen zero and 1\n",
    "##R2=1-MSE of regression line/MSE of the average of the data\n",
    "##R2 near 1 then line good fit for data\n",
    "##R2 near zero line didnot perform well\n",
    "##R2 using score method in linear regression\n",
    "##R2 negatiev due to overfitting\n",
    "\n",
    "##Prediction and Decision Making\n",
    "##MSE is one of the way for evaluate if model is good or not\n",
    "##also R2 to evaluate model\n",
    "\n",
    "##lower MSE do not always mean a good fit\n",
    "##MSE of MLR model will be smaller then MSE of SLR model since error of data is decrese due to more variable included\n",
    "##Polynimal Regression has lower MSE then regular regression.\n",
    "\n",
    "## for R2 inverse relationship ..R2 of SLR lower then MLR and R2 of Polynomial greater then regular regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation and Refienment\n",
    "#Model Evaluation tell us how our model perform in real world\n",
    "##in sample evaluation..how well our model will fit the data used to train it\n",
    "##but it does not tell how well the trained model can be used to predict data\n",
    "##solution to split data..train aand test set\n",
    "##Generalization Error is measure of how well our data does at predicting previously unseen data\n",
    "##Error we obtain using our testing data is an approximation of this error.\n",
    "##to overcome generalization error we use cross validation\n",
    "##most common out of sample evaluation metrics\n",
    "##more effective use of data(each observation is used for both training and testing)\n",
    "\n",
    "##from sklearn.model_selection import cross_val_score\n",
    "##..cross_val_score is basically R2 value for each fold\n",
    "##score=cross_val_score(lr,x,y,cv=4)\n",
    "##np.mean(score)\n",
    "##cross_val_predict()...returns the prediction for each element when it was in the test set\n",
    "##from sklearn.model_selection import cross_val_predict\n",
    "##yhat=cross_val_predict(lr,x,y,cv=4)\n",
    "\n",
    "##Overfitting ,underfitting and model selection\n",
    "##test=[]\n",
    "##order=[1,2,3,4]\n",
    "##for n in order:\n",
    "##  pr=PolynomialFeatures(degree=n)\n",
    "##  x_train=pr.fit_tranform(x_train[[\"horsepower\"]])\n",
    "##  x_test=pr.fit_transform(x_test[[\"horsepower\"]])\n",
    "##  lr.fit(x_train,y_train)\n",
    "\n",
    "##Ridge Regression\n",
    "##prevents overfitting\n",
    "##In order to determine parameter alpha we use data for training..we use second set called validation data..similar to test data\n",
    "##but it is used to select parameter like alpha..we calculate R2 for different value of alpha...select alpha with better r2.\n",
    "##as alpha increses r2 for validation increases but for test set..with increse in alpha r2 decreses.\n",
    "\n",
    "##Grid Search..allows us to scan through multiple free parameters with few line of code.\n",
    "##alpha term in ridge regression called hyper parameter\n",
    "##Scikit learn  has a means of automatically iterating over this hyper parameter using cross validation called grid search.\n",
    "## to select our hyper parameter we divide the data in 3 parts..traningset,validation set,test set\n",
    "##train mode; with different hyper parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
