{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 1...Introduction and history of Deep Learning\n",
    "\n",
    "#Deep Learning  subset of Machine Learning which in turn is a subset of AI.\n",
    "\n",
    "#Idea of Deep Learning started in 1984....idea of perceptron inspired by neuron..neuron basic building block of brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First step summation...We give weights to input and some input may have more weight as they may be more important\n",
    "#Xnet-x1w1+x2w2+x3w3 (x1,x2,x3 are input) (w1,w2,w3 are weights)\n",
    "\n",
    "#Second step is activation function....give us the output\n",
    "\n",
    "#Third Step Output is y\n",
    "\n",
    "#structure of perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem with perceptron or Node\n",
    "# 1. It has single perceptron or Node or neuron\n",
    "# 2. No learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 1980,So next big invention MLP (Multi layer perceptron)\n",
    "\n",
    "#Structure of MLP\n",
    "#Input Layer\n",
    "#Hidden Layer or Nodes or perceptron\n",
    "#Output Layer\n",
    "\n",
    "#Hidden layer ..perceptron stacked one above the other in a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#due to less computational power our computers were not capable to prove in practical and also due to less of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In 2012...also known as modern era..we have high computational power and huge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video-2.... structure of neuron and MLP\n",
    "#Bias and weighs are trainable parameter as they will keep on changing while we are training \n",
    "#so that model finally understand which input is how much important \n",
    "#as weigts can be controlled not inputs\n",
    "#weight called trainable or learnable\n",
    "#weights n bias are unique\n",
    "\n",
    "#As this process is moving forward called forward feed\n",
    "\n",
    "#first part they get summed up\n",
    "#then they have activation function\n",
    "#get the outputs which act as input for nodes in next layer\n",
    "#all weight saved as metrices where first dimension equal to no of input and second no of output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video 3....Optimisers-Gradient Descent.\n",
    "\n",
    "#we have to adjust weights and bias so that predicted value is close to actual value\n",
    "#diffrence between predicted value actual value is loss.\n",
    "#We want low loss so we use optimization technique\n",
    "#we have to optimize weight and bias to have minimum loss\n",
    "\n",
    "#Gradient descent is a basic optimizer\n",
    "#W(new)=W(old)-dl/dw...differentiation of loss/differentiation of weight\n",
    "\n",
    "#magnitude of descent is called learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video 4...Back propagation\n",
    "#known as chain rule of differentiation\n",
    "# Together forward and backward propagation known as 1 epoch\n",
    "#this epoch are done one after the other so that we reach ideal model where we have minimum loss\n",
    "# many epoch lead to training of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#video 5...Activation functions in depth.\n",
    "\n",
    "#Sigmoid function a=1/1+exp(-z)...it lie between 0 to 1...useful in classification..logisctic regression\n",
    "#Derivative of sigmoid function...is a(1-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tanh Function\n",
    "# a=e^z - e^-z/e^z + e^-z....tanh lie between -1 to 1\n",
    "#Derivate is 1-a^2.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReLU Function..Rectified Linear Units\n",
    "#Sigmoid n Tanh has lot of calculation so RelU\n",
    "# a= max(0,Z)\n",
    "#so RelU has problem becuase derivative of postive number will be positive but negative number will be zero\n",
    "\n",
    "#Problems with LeRU\n",
    "#Dying Neurons..every time we have a summation(z) value greater than zero we can always get one..so while differentiation\n",
    "#we will get one but for case z less than zero we get differentiation zero...so we don't get new weights so this problem is called\n",
    "#dying neuron problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem with Sigmoid ...give value between 0 to 1 and lot of calculation\n",
    "#Problem with Tanh..lot of calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To overcome dying neuron problem..Leaky ReLu \n",
    "#Fucntion of Leaky ReLU a=max(0.01z,z)\n",
    "#Derivative..0.01 if z<0 and z if z>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in leaky ReLu we were taking 0.01 but why.... so parameterized RELU\n",
    "#Parameterized ReLU\n",
    "# a=max(alphaZ,Z)..where alpha can be any number trained by the model..if model decide alpha is 0.01 so it is 0.01\n",
    "#Model decide what is the best number to multiply z with\n",
    "#Derivate..alpha if z<0 and 1 if z>=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELU....Exponential ReLU\n",
    "# function is alpha(e^z-1) if z<0 and z if z>0\n",
    "#Derivate......alpha(e^z) for z<0 and 1 for z>=0\n",
    "\n",
    "#One problem that ReLU always had is that it is not differentiable at zero..so we don't know what the value will be at zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Swish function...paper released by google researcher\n",
    "# bring the good version of ReLU and sigmoid together to form swish\n",
    "# a=z*sigmoid(z)\n",
    "#derivate will be sigmoid (z) + z*sigmoid(z)(1-sigmoid(z))\n",
    "\n",
    "#in swish function 0 is differentiable so we get value at zero as well\n",
    "#In imagenet task gave extra 1% of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax Activation Function\n",
    "#Used lot for multiclass classification\n",
    "#Formula for activation softmax function..a=e^z/summation of e^z...lie between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 6..Where to use which activation.\n",
    "# Promblems with activation function\n",
    "\n",
    "#Sigmoid function between 0 to 1 so less focus when negative value..another problem hard to compute\n",
    "#Tanh..give  value from -1 to 1 ... problem..hard to compute\n",
    "#another from is Vanishing Gradient....\n",
    "#In back propagation..we have sigmoid value value as 0.1 ..so suppose we have 3 ouput in backpropagation so .1*.1*.1=.001\n",
    "#we then use it...out etta is 0.1 so w(new)=w(old)-etta*.0001\n",
    "#suppose etta is also..0.1...so w(new) will be very similar to w(old)\n",
    "#Due to this the gradient we r calculating r so small that multiplying one wth other will result actual value vanishing.\n",
    "#If w(new) very close to w(old) then we will never rech to lowest minima.\n",
    "#So this is problem we had with sigmoid and tanh so RiLU was introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so problem with RiLU...dying RiLU problem...\n",
    "# RiLU activation function a= max(0,Z)\n",
    "#so RelU has problem becuase derivative of postive number will be positive but negative number will be zero.\n",
    "# so IF value greater than zero always get differentiation as 1\n",
    "#if value less than zero then differentiation is zero...so dl/dw=0\n",
    "#so w(new)=w(old)-etta*0\n",
    "# w(new)=w(old)\n",
    "#to overcome thsi problem we have parameterized RiLU and LeaKY RiLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another problem with RiLU ..as it is not continous function so while differentiating not able to differentiable at zero\n",
    "#So swiss function to overcome this problem\n",
    "#Swiss function was very simple thing a=z*sigmoid(z)\n",
    "#it took best of both ReLU and sigmoid \n",
    "#In imagenet task gave extra 1% of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So where to use which function\n",
    "\n",
    "#Sigmoid and Tanh should avoided because of vanishing gradient\n",
    "\n",
    "#Can use RiLU for most problems..versitle..Mostly used in hidden layers so do not use RiLU in input or output layer\n",
    "#if RiLU does not work then Leaky RiLU and parameterized RiLU\n",
    "# Softmax or Sigmoid mostly preferred in classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 7..Optimisers\n",
    "#Gradient Descent optimiser of 3 types\n",
    "#Batch Gradient Descent...Mini Batch dradient Descent...Stochastic Gradient Descent\n",
    "#formula for all three w(new)=w(old)-Etta(dL/dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch Gradient descent\n",
    "#It calculate the dL/dw for all the datapoints..suppose suppose 10000 data points.\n",
    "#after calculating it ..it averages it out and updates the weight\n",
    "#Problem...it calculate gradient descent for all datapoints it become very slow\n",
    "\n",
    "#Mini Batch gradient Descent\n",
    "#To overcome the problem of batch gradient descent\n",
    "#As the name suggest it will not use entire datapoint but differentiation will be in mini batch\n",
    "#suppose 100 data points so differentiation of 100 datapoints so it will randomly select the 100 datapoints and will\n",
    "#calculate the differentiation of 100 datapoints and will update the weight.\n",
    "\n",
    "#Stochastic Gradient Descent\n",
    "#It randomly select 1 datapoint,differentiation of 1 datapoint and updates the weight on basis of that.\n",
    "#take very long time to achieve minima\n",
    "\n",
    "#Mini Batch gradient descent most used gradient descent among the three\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To avoid so many data points and reduce time with out being haphazard so we use Stochastic Gradient Descent with momentum\n",
    "#Or Mini Batch gradient Descent with Momentum\n",
    "\n",
    "#Moementum means giving weightage to previous gradient descent so that next gradient descent is not haphazard as previous one\n",
    "#Gamma always between 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 8..Optimisers\n",
    "#Adagrad Optimiser\n",
    "#Main idea behind Adagrad was to change the learning rate instead of gradient descent\n",
    "#so change the learning rate with each epoch or iteration\n",
    "#So initally in graident descent to achive minima taek larger step and as we approach the minima take smaller steps\n",
    "#this tranformation from larger step to smaller step will be with each iteration by changing the learning rate.\n",
    "\n",
    "#Problem with Adagrad..Etta was decreasing very quickly as summation of gradient is squared so if 1 gradient (i=1) so it is small\n",
    "#but as gradient step increases so summation of square increases which is in denominator so Etta(dash) decrases quickly\n",
    "# w(t)=w(t-1)-Etta(dash)g(t)\n",
    "#Etta(dash)=Etta/sqrt(alpha(t-1)+epsilon)\n",
    "\n",
    "#alpha(t-1)=summation of all gradient square from i=1 to i=t-1\n",
    "\n",
    "#For every new iteration which bring closer to minimum loss Etta(dash) decreases\n",
    "#Etta(dash) decrease very quickly so there might be a time when Etta(dash) will be very close to zero so w(t) very close to w(t-1)\n",
    "# so weights will be very similar so no update in weight so might not reach to minimum loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to overcome the procome Adagrad we have AdaDelta\n",
    "#AdaDelta optimisers\n",
    "# Idea is same but tries to solve the problem of fast decreasing in learning rate\n",
    "# w(t)=w(t-1)-Etta(dash)g(t)\n",
    "# Etta(dash)=Etta/sqrt(eda(t-1)+epsilon)\n",
    "\n",
    "#eda(t-1) = gamma*eda(t-2) + (1-gamma)*(g(t-1))^2\n",
    "#eda=exponential decaying average\n",
    "#so denominator will not increase very fast so Etta(dash) will not decrease very fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam (Adaptive Moment Estimation) Optimiser\n",
    "#Most Commonly used optimiser\n",
    "#Adam came up with was along with using gradient squares will use gradients..so eda of gradient square and gradient\n",
    "# m(t)=beta1*m(t-1) + (1-Beta)*g(t)\n",
    "#it kind of eda or momentum term for m(t)\n",
    "#Adam proven to be very good method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#when to which which optimiser\n",
    "# if spare data like data from bag of words...go for Ada function..most likely Adam\n",
    "\n",
    "#General Rules\n",
    "#Adam most commonly used\n",
    "#if spare data like data from bag of words...go for Ada function.Adagrad..AdaDelta or Adam....most likely Adam for spare data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 8..Weight Initialisers\n",
    "#Weights were randomly assigned but from a distribution\n",
    "#Weight should neither be too small nor too big..becoz..if differentiation is too small so weight won't get updated\n",
    "#that is vanishing gradient problem when weight is too small\n",
    "#if weight too big we come across a problem of gradient explosion or exploding gradient\n",
    "#So we need to clip this weights that is keep them in certain range so for that we need distribution\n",
    "#we use weight from the distribution.\n",
    "\n",
    "#Weight Should not be equal becoz if weights are equal then new weight will also be same so it is difficult to define which\n",
    "#input must be given more weightage.\n",
    "# Fan  in number of input\n",
    "# Fan out number of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NormalDistribution...Normal Initialization..all the weights of this format will be intialized from normal initialization N(0,Sigma)\n",
    "#Centre at zero to give enough weightage to negative weights so this is not at all related in any way to output we are getting.\n",
    "\n",
    "#Uniform Distribution...Uniform Initialization...Weights will be initialized from a uniform distribution which lie between -1/sqrt(fanin) to 1/sqrt(fanin)\n",
    "#here distribution is uniform\n",
    "\n",
    "#Xavier/Glorot Initialization... \n",
    "#Two different type of initialization..weight lie between Normal distribution...N(0,Sigma)....Sigma=2/fanin+fanout\n",
    "#Here for first time input and output is considered that fanin n fanout\n",
    "\n",
    "#Xavier/Glorat Initialization from uniform distribiution\n",
    "#here weights are intialized from uniform distribution which lie between -sqrt(6)/sqrt(fanin+fanout) , +sqrt(6)/sqrt(fanin+fanout)\n",
    "\n",
    "#He Initialization \n",
    "#two type\n",
    "#Normal one and unuform one\n",
    "#Normal one..N(0,Sigma)...Sigma=sqrt(2/fanin)\n",
    "#uniform one...-sqrt(6/fanin) , +sqrt(6/fanout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal and uniform distribution totally deoend on what type of function we are using\n",
    "# He intialization...usually used with activation function like ReLU, Leaky ReLU or any other type of ReLU\n",
    "# Xavier or Glorot Initialization usually used for sigmoid activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 10...Losses in Neural networks.\n",
    "#Different Loss function of neural networks\n",
    "#Regression Problem n Classification Problem\n",
    "#Classification Problem two type..binary n multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Losses..\n",
    "#Mean Squared Error...for i=1 to N..Mean of (y(i)-yhat(i))^2...i is datapoint\n",
    "#MSE is weak when it comes to finding outlier..because error is squared..so actual loss is skewed\n",
    "\n",
    "#Mean Absolute Error..L1 Loss..for i=1 to N...Absolute(y(i)-yhat(i))/N\n",
    "#MAE is good at finding outlier..find absolute difference but very small errors will not allow model to learn\n",
    "\n",
    "#Huber Loss..combination of MSE n MAE\n",
    "#Huber Loss..1/2(y(i)-yhat(i))^2 when mode of (y(i)-yhat(i))<=Delta\n",
    "#Otherwise Delta times mode of (y(i)-yhat(i)) - (Delta)^2/2\n",
    "#Problem of Huber loss is deciding value of Delta which is a daunting task so it is not much in use\n",
    "\n",
    "#log cosh...good function not used much\n",
    "# log(cosh(y(i)-yhat(i))\n",
    "#for very small error..this function behaves like squared function small(y-yhat)=(y(i)-yhat(i))^2/2\n",
    "#for large value of error...Absolute(y(i)-yhat(i))-log(2)\n",
    "\n",
    "#Root Mean Squre Error is L2 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Losses\n",
    "\n",
    "#Binary Classification loss\n",
    "#Binary Cross Entropy..label are 0 n 1\n",
    "#BCE=-1/N(Summation  from 1 to N for ylog(yhat) + (1-y)log(1-yhat))..basically calculating the mean value of all the labels\n",
    "#entropy=ylog(y)\n",
    "#Cross entropy=ylog(yhat)\n",
    "\n",
    "#Hinge Loss..labels are -1 n 1\n",
    "# max(0,1-yhat*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultiClass Classification Loss\n",
    "#Categorical cross entrophy is   summation of i from 1 to c(no of classes) ylog(yhat)\n",
    "\n",
    "#KL Divergence..used when we have 2 very different distribution or 2 different proability like\n",
    "#uniform distribution and binomial distribution\n",
    "#so we need to find out how different is each distribution from the other\n",
    "#KL diverhence used in generative model\n",
    "#we have other divergence as JS divergence,earth movers divergence,variational auto encoder(VAE),GAN basically used for deep fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video 11 Handwritten number recognition using Keras part 1\n",
    "#Maulti Layer Perceptron\n",
    "#Coding a multilayer perceptron\n",
    "# 3 main framework for coding in deeplearning\n",
    "# it is not easy for us to write code for every thing like.losses,activation function,weight intialization so\n",
    "#this framework make it easy for us to code we dont have to write each and everything\n",
    "#3 main framework..Tensorflow..Keras..Pytorch\n",
    "#Pytorch most relatively new released by facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#difference between Keras and Tensorflow\n",
    "\n",
    "#Tensorflow..Low level framework(write more code)..NotBegineerfriendly.. or Userfriendly..second most used..debugging not soeasy\n",
    "#Tensorflow is faster build on C n C++  n they r faster...\n",
    "#Tensorflow is flexible..so making change in code is easy\n",
    "\n",
    "#Keras..high level framework(write leaa code.)..Begineerfriendly...Userfriendly..Most used framework..Easy to debug\n",
    "#Keras build on python so slow\n",
    "#Keras not so flexible..making change in code is not so easy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\priyanka\\appdata\\roaming\\python\\python37\\site-packages (21.2.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\python 2\\lib\\site-packages (2.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.6.0-cp37-cp37m-win_amd64.whl (423.2 MB)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\python 2\\lib\\site-packages (from tensorflow) (0.37.0)\n",
      "Collecting tensorflow-estimator~=2.6\n",
      "  Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Collecting google-pasta~=0.2\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in c:\\python 2\\lib\\site-packages (from tensorflow) (1.39.0)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Using cached wrapt-1.12.1-py3-none-any.whl\n",
      "Requirement already satisfied: keras~=2.6 in c:\\python 2\\lib\\site-packages (from tensorflow) (2.6.0)\n",
      "Collecting termcolor~=1.1.0\n",
      "  Using cached termcolor-1.1.0-py3-none-any.whl\n",
      "Collecting h5py~=3.1.0\n",
      "  Using cached h5py-3.1.0-cp37-cp37m-win_amd64.whl (2.7 MB)\n",
      "Collecting clang~=5.0\n",
      "  Using cached clang-5.0-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\python 2\\lib\\site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\python 2\\lib\\site-packages (from tensorflow) (1.19.5)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\python 2\\lib\\site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\python 2\\lib\\site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\python 2\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard~=2.6\n",
      "  Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: cached-property in c:\\python 2\\lib\\site-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\python 2\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\python 2\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python 2\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\python 2\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\python 2\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python 2\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (2.26.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\python 2\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\python 2\\lib\\site-packages (from tensorboard~=2.6->tensorflow) (0.4.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\python 2\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\python 2\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\python 2\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\python 2\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\python 2\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in c:\\python 2\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python 2\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python 2\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python 2\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2018.8.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python 2\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.23)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\python 2\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\python 2\\lib\\site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.5.0)\n",
      "Installing collected packages: wrapt, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, h5py, google-pasta, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.10.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n",
      "ERROR: Cannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\python 2\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\python 2\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#Code in Keras..task to create multilayer perceptron\n",
    "#Problem statement..work on MNIST dataset\n",
    "#in MNIST dataset we have some images of 28*28 dimensions and we have hand written number in it.\n",
    "# we have to identify that hand written number and picture..n.which class it belong to from 0 to 9.\n",
    "\n",
    "\n",
    "\n",
    "!pip install --user --upgrade pip\n",
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b0a319c7b841>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python 2\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Please release your cursor ok"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
