{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Assumption of linear regression\n",
    "##Linear model is linear in terms of coefficient and error term.\n",
    "##the mean of residual is zero\n",
    "##the error terms are not correlated with eachother\n",
    "##the independent variable are uncorrelated with the residual term\n",
    "##the error term  has a constant variance\n",
    "##No multicollinearity\n",
    "##the error term are normally distribuated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Residuals....Error (Sum(yactual-Ypredicted))\n",
    "##Best fit line..pass through all the point and minimize the residual\n",
    "##Linear Regression=y=mx+c...m slope or coefficient...x is intercept\n",
    "##residual(r)=y(actual)-mx+c(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Simple linear regression\n",
    "##Simple linear regression is  a method for predicting quantitative response using a single feature\n",
    "##y=Bo+B1x\n",
    "##y=mx+c\n",
    "##y is the response or target variable\n",
    "##x is feature\n",
    "##Bo is coefficient of x\n",
    "##B1 is intercept\n",
    "##Bo and B1 are mode; coefficient\n",
    "##To craete a model we must learn the vlaue of these coefficient.once we have the value of these coefficent we use the model\n",
    "##to predict the sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import pandas as pd\n",
    "##import matplotlib.pyplot as plt\n",
    "##import pickle\n",
    "##%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data=pd.read_csv(\"advertising.csv\")\n",
    "##data.head()\n",
    "##x_new=pd.DataFrame({\"TV\":[50]})\n",
    "##x_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create x and y\n",
    "##feature_cols=[\"TV\"]\n",
    "##x=data[feature_cols]\n",
    "##y=data.sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from sklearn.linear_model import LinearRegression\n",
    "##lr=LinearRegression()\n",
    "##lr.fit(x,y)\n",
    "##lr.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Gradient Descent\n",
    "##how a model is going to minimize the error\n",
    "##Gradient descent is error and descent is decrease...so decrease error\n",
    "##how models are are trying to decrease erroe after period of time i.e. multiple iteration\n",
    "##cost function reaches global minima..with the help of learning rate\n",
    "##When we can call we have optimized the cost function (error) only when the cost finction reaches the global minimum\n",
    "##Gradient descent is an optimization algorithm used to find the values of coefficients of a function (f) that minimizes a error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Multiple inear Regression\n",
    "##Create a model to see the relationship between multiple features and label column.This called mutiple linear regression\n",
    "##y=B0+B1x1+B2x2.....+Bnxn\n",
    "##import pandas as pd\n",
    "##import matplotlib.pyplot as plt\n",
    "##import pickle\n",
    "##%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data=pd.read_csv(\"Advertising.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create x and y\n",
    "##feature_cols=[\"TV\",\"radio\",\"newspaper\"]\n",
    "##x=data[feature_cols]\n",
    "##y=data.sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from sklearn.linear_model import LinearRegression\n",
    "##lr=LinearRegression()\n",
    "##lr.fit(x,y)\n",
    "##lr.predict([50,20,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Standard Scaler\n",
    "##bigger digit may get more importance so to avoid bias we use standard scaler\n",
    "##standard scaler will bring feature with unit into unitless..will standardize the data\n",
    "##will normalize the data where mean=0 and standard deviation=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##x=pd.DataFrame({2000,2,100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from sklearn.preprocessing import StandardScaler\n",
    "##scaler=StandardScaler()\n",
    "##x_scaled=scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##print(x_scaled[0],x_sclaed[1],x_scaled[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Formula for standard scaler\n",
    "##z=(x-mean)/std\n",
    "##import numpy as np\n",
    "##mean=np.mean(x)\n",
    "##std=np.std(x)\n",
    "##z0=x0-mean/std,z1=x1-mean/std,z2=x2-mean/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##StandardScaler removes the mean\n",
    "##StandardScaler scales each feature/variable to unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Train-Test split\n",
    "##train test split is hold put methodS\n",
    "##data=pd.read_csv(\"admission_prediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##y=data[\"chance of admit\"]\n",
    "##x=data.drop(columns=[\"chance of admit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from sklearn.model_selection import train_test_split\n",
    "##xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##xtrain.shape\n",
    "##ytrain.shape\n",
    "##xtest.shape\n",
    "##xtest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Linear Regression model and evalution metrics\n",
    "##import pandas as pd\n",
    "##import numpy as np\n",
    "##import matplotlib.pyplot as plt\n",
    "##import seaborn as sns\n",
    "##from sklearn.linear_model import LinearRegression\n",
    "##from sklearn.model_selection import train_test_split\n",
    "##from sklearn.metrics import mean_squared_error,mean_alternate_error,r2_score\n",
    "##from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data=pd.read_csv(\"Admission_prediction.csv\")\n",
    "##data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data[\"universityrating\"]=data[\"universityrating\"].fillna(data[\"universityrating\"].mode()[0])\n",
    "##data[\"TOEFLscore\"]=data[\"TOEFLscore\"].fillna(data[\"TOEFLscore\"].mean())\n",
    "##data[\"GREscore\"]=data[\"GREscore\"].fillna(data[\"GREscore\"].mean())\n",
    "##data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data=data.drop(columns=[\"SerialNo\"])\n",
    "##data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.figure(figsize=(20,25),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##for column in data:\n",
    "##    if plotnumber<=16:\n",
    "##        ax=plt.subplot(4,4,plotnumber)\n",
    "##        sns.distplot(data[column])\n",
    "##        plt.xlabel(column,fontsize=20)\n",
    "##    plotnumber+=1\n",
    "##plot.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##y=[\"chanceof admit\"]\n",
    "##x=data.drop(columns=[\"\"chanceof admit\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.figure(figsize=(20,30),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##for column in x:\n",
    "##    if plotnumber<=15:\n",
    "##        ax=plt.subplot(5,3,plotnumber)\n",
    "##        plt.scatter(x[column],y)\n",
    "##        plt.xlabel(column,fontsize=20)\n",
    "##        plt.ylabel(\"chanceofadmit\",fontsize=20)\n",
    "##    plotnumber+=1\n",
    "##plot.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ss=StandardScaler()\n",
    "##x_scal=ss.fit_trandorm(x)\n",
    "##xtrain,xtest,ytrain,ytest=train_test_split(x-cal,y,test_size=.25,random_state=42)\n",
    "##y_train\n",
    "##lr=LinearRegression()\n",
    "##lr.fit(xtrain,ytrain)\n",
    "##lr.score(xtrain,ytrain)\n",
    "##lr.coefficient_\n",
    "##lr.intercept_\n",
    "##pred=lt.predict(xtest)\n",
    "##r2_score(ytest,pred)\n",
    "##mean_squared_error(ytest,pred)\n",
    "##mean_absolute_error(ytest,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##r2_score ..range from 0 to 1...can multiply by 100 for percentage\n",
    "##lr.score provide model score or performance from 0 to 1....can multiply by 100 for percentage\n",
    "##adjusted r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Regularization\n",
    "##to avoid overfitting problem\n",
    "##Regularization helps sort the problem of overfitting by restricting the degree of freedom of a gievn equation\n",
    "##Simply reducing the number of degree of freedom of a polynomial function by reducing their corresponding weights\n",
    "##In linear equation we do not want hugh weight/coefficient as small change in weight can make a large difference for the\n",
    "##dependent variable.So regularization constraints the weight of such feature to avoid overfitting\n",
    "##Simple linear regression=B0+B1x1+B2x2....Bpxp\n",
    "##L1=Lasso...if some feature f1 is contributing not much while pridicting the target/label then lasso will not atall consider it\n",
    "##will make it zeroor ignore it .suppose f2 is adding to much or have to much weightage in predicting the target then it will add penalty to reduce \n",
    "##its weightage.We can find penalty with help of lambda so what lambda or penalty data must be selected is provided by LassoCv\n",
    "##lassocrossvalidation...\n",
    "##L2=Ridge....if some feature f1 is contributing not much while pridicting the target/label then Ridge will give \n",
    "##very less importance to that feature.suppose f2 is adding to much or have to much weightage in predicting the target then it will add penalty to reduce \n",
    "##its weightage.We can find penalty with help of lambda so what lambda or penalty data must be selected is provided by RidgeCv\n",
    "##Ridgecrossvalidation...\n",
    "##lamda in regularization is learning rate\n",
    "##regularization techniques..Ridge,Lasso and Elastic Net\n",
    "##we use Regularization...To find if the model is overffitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import panda s pd\n",
    "##import numpy as np\n",
    "##from sklearn.preprocessing  import StandardScaler\n",
    "##from sklearn.liner_model import Lasso,LassoCV,Ridge,RidgeCV\n",
    "##from sklearn.model_selection import train_test_split\n",
    "##from sklearn.metrics import r2_score\n",
    "##import seaborn as sns\n",
    "##import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lasso Regularization\n",
    "##LassoCV will return best alpha and coefficents after performing 10 cross validation\n",
    "##lassocv=LassoCV(alpha=None,cv=10,max_iter=100000,normalize=True)\n",
    "##Lassocv.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##best alpha parameter\n",
    "##alpha=Lassocv.alpha_\n",
    "##alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now using alpha\n",
    "##lasso_reg=Lasso(alpha)\n",
    "##Lasso_reg.fit(x_train,y_train)\n",
    "##Lasso_reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ridge Regression model\n",
    "##RidgeCV willreturn best alpha and coefficents after performing 10 cross validation\n",
    "##we will pass an array of random numbers for ridgeCV to select best alpha from them\n",
    "##alphas=np.random.uniform(low=0,high=10,size=(50,))\n",
    "##ridgecv=RidgeCV(alphas=alphas,cv=10,normalize=True)\n",
    "##ridgecv.fit(x_train,y_train)\n",
    "##alpha=ridgecv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ridge_Reg=Ridge(alpha)\n",
    "##Ridge_Reg.fit(x_train,y_train)\n",
    "##Ridge_Reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##overfitting n Underfitting\n",
    "##overfitted model..during training providing accuracy of 95% or 97% or 100% but during testing providing very low accuracy\n",
    "##such modela re overfitted model.I want model to be generalized that accuracy during training and testing must be similar\n",
    "## to avoid overfitting use regularization\n",
    "## overfitting,,When we provide too many features for model training n When training and testing data are most similar\n",
    "\n",
    "##Underfitted model...suppose 5 features are important for prediction but we are using 2 features and ignoring other\n",
    "##so model is not perform well..accuracy will be suppose 50%\n",
    "##less feature to build a model can result in underfitted mdoel\n",
    "##to avoid taht we wiil check the relationship between features and target variable or label\n",
    "##underfitting...When we provide very less features for model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sigmoid function..are the underlying function in logistic regression\n",
    "##sigmoid function range is between 0 to 1.thus it is useful for calculating probability for the logistic fucntion.\n",
    "##it's derivative is easy to calculate than other function which is useful during gradient descent calculation.\n",
    "##simple way of introducing non linearity to the model.\n",
    "##although there are other fucntions as well which can be used,but sigmoid is the most common fucntion for logistic regression\n",
    "##the intution behind logistic regression is to classify the label or a class..which class a particular data belong to 0 or 1\n",
    "##binary classification...how dowe decideor classify a data is o or 1\n",
    "##if probability  greater than .5 is 1 and less than .5 is 0\n",
    "##formula for sigmoid function....1/1+power(e) is -t...where t is mx+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "##assumption of logistic regression\n",
    "##a datapoint must be linearly separable\n",
    "\n",
    "##logistic function formula p(X)= power of e(B0+B1X)/1 + power of e(B0+B1X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Multinomial Logistic Regression\n",
    "##if classes are greater than 2 or category in that case we use multinomial logistic regression\n",
    "##category do not need to be in order\n",
    "## multinomial logistic regression...When labels have more than two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Variance Inflation factor\n",
    "##can import from\n",
    "##VIF compares one predictor eith another predictor and will try to find the score\n",
    "##suppose we have 100 features then how will we indentigy which feature is related to which feature\n",
    "##so we use variance inflaon factor\n",
    "##check each and every predictor with another predictor\n",
    "##if related then call it multicollinearity problem\n",
    "##we have to avoid the multi collinearity in our dataset.\n",
    "##VIF can be applied only on predictor not on label/target\n",
    "\n",
    "##import numpy as np\n",
    "##import pandas as pd\n",
    "##from sklearn.preprocessing import StandardScaler\n",
    "##from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data=pd.read_csv(\"diabetes.csv\")\n",
    "##data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##x=data.drop(column=[\"outcome\"])\n",
    "##y=data[\"outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##scaler=StandardScaler()\n",
    "##x_scaler=scaler.fit_transform(x)\n",
    "##x_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vif=pd.DataFrame()\n",
    "##vif[\"vif\"]=[variance_inflation_factor(x_scaler,i) for i in range(x_scaler.shape[1])]\n",
    "##vif[\"Features\"]=x.columns\n",
    "\n",
    "##let's check the value\n",
    "##all the vif value are less than 5 and are very low.So no multicolinearity.so now we can split our data\n",
    "##if suppose 2 column have vif more than 5 then we can delete one of column..also those 2 column are related and have multicolineairy\n",
    "##which column to delete out of 2..will be determined by p value..higher p value we will keep that column and delete the other one\n",
    "##some people will use vif threshhold as 5 or may use 10 also..no fix number..depend on type of dataset and variance in dataset\n",
    "##vif help identify multicollinearity and help in feature seelction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vif=1/1-R**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Logistic regression model building\n",
    "##import pandas as pd\n",
    "##import numpy as np\n",
    "##import matplotlib.pyplot as plt\n",
    "##import scipy.stats as st\n",
    "##import seaborn as sns\n",
    "##from sklearn.preprocessing import StandardScaler\n",
    "##from sklearn.model_selection import train_test_split\n",
    "##from sklearn.linear_model import LogisticRegression\n",
    "##from statsmodels.stats.outliers_influence import variance inflation_factor\n",
    "##from sklearn.metrics import acccuracy_score,classification_report,confusion_matrix,roc_curve,auc_roc_score\n",
    "##sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data=pd.read_csv(\"diabetes.csv\")\n",
    "##data.head()\n",
    "##data,describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.figgure(figsize=(20,25),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##..to check skewness..we don't have to touch label/target\n",
    "##for column in data:\n",
    "##    if plotnumer<=9:\n",
    "##        ax=plt.subplot(3,3,plotnumber)\n",
    "##        sns.distplot(data[column])\n",
    "##        plt.xlabel(\"salary\",fontsize=30)\n",
    "##    plotnumbe=plotnumber+1\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##null values\n",
    "##data[\"BMI\"]=data[\"BMI\"].replace(0,data[\"BMI\"].mean())\n",
    "##data[\"BloodPressure\"]=data[\"Bloodpressure\"].replace(0,data[\"Bloodpressure\"].mean())\n",
    "##data[\"glucose\"]=data[\"glucose\"].replace(0,data[\"glucose\"]).mean())\n",
    "##data[\"insulin\"]=data[\"insulin\"].replace(0,data[\"insulin\"]).mean())\n",
    "##data[\"skinthickness\"]=data[\"skinthickness\"].replace(0,data[\"skinthickness\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now again check the distribution\n",
    "##plt.figgure(figsize=(20,25),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##..to check skewness..we don't have to touch label/target\n",
    "##for column in data:\n",
    "##    if plotnumer<=9:\n",
    "##        ax=plt.subplot(3,3,plotnumber)\n",
    "##        sns.distplot(data[column])\n",
    "##        plt.xlabel(\"salary\",fontsize=30)\n",
    "##    plotnumbe=plotnumber+1\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##skewness\n",
    "##q=data[\"pregnancies\"].quantile(0.98)\n",
    "##we are removing the top 2% data from pregnancies column\n",
    "##data_cleaned=data[data[\"pregnancies\"]<q]\n",
    "##q=data_cleaned[\"BMI\"].quantile(0.99)\n",
    "##we are removing the top 1% data from BMI\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"BMI\"]<q]\n",
    "##q=data_cleaned[\"skinthickness\"].quantile0(.99)\n",
    "##we are removing the top 1% data from skinthickness\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"skinthickness\"]<q]\n",
    "##q=data_cleaned[\"insulin\"].quantile0(.99)\n",
    "##we are removing the top 1% data from insulin\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"insulin\"]<q]\n",
    "##q=data_cleaned[\"diabetespedigreefunction\"].quantile(0.99)\n",
    "##we are removing the top 1% data from diabetespedigree\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"diabetespedigreefunction\"]<q]\n",
    "##q=data_cleaned[\"age\"].quantile(0.99)\n",
    "##we are removing the top 1% data from age\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"age\"]<q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##once again check the distribution\n",
    "##plt.figgure(figsize=(20,25),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##..to check skewness..we don't have to touch label/target\n",
    "##for column in data_cleaned:\n",
    "##    if plotnumer<=9:\n",
    "##        ax=plt.subplot(3,3,plotnumber)\n",
    "##        sns.distplot(data_cleaned[column])\n",
    "##        plt.xlabel(\"salary\",fontsize=30)\n",
    "##    plotnumbe=plotnumber+1\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=data_cleaned.drop(columns=[\"outcome\"])\n",
    "#y=data_cleaned[\"outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's see how data is distributed for every column\n",
    "##plt.figure(figsize=(20,25),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##for column in x:\n",
    "##   if plotnumber<=9\n",
    "##      ax=ply.subplot(3,3,plotnumber)\n",
    "##      sns.stripplot(y,x[column])\n",
    "##   plotnumber=plotnumber+1\n",
    "##plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ss=StandardScaler()\n",
    "##x_scaled=ss.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vif=pd.DataFrame()\n",
    "##vif[\"vif\"]=[variance_inflation_factor[x_scaled,i] for i in range(x_scaled.shape[1])]\n",
    "##vif[\"features\"]=x_scaled.columns\n",
    "##vif\n",
    "##vif is less than 5 so no multicolinearity\n",
    "##xtrain,xtest,ytrain,ytest=train_test_split(x_sclaed,y,test-size=.25,random_state=355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lg=LogisticRegression()\n",
    "##lg.fit(xtrain,ytrain)\n",
    "##pred=lg.fit(xtest)\n",
    "##lgscore=accuracy_score(ytest,pred)\n",
    "##lgscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Confusion MAtrix\n",
    "##Accuracy=TP+TN/TP+TN+FP+FN\n",
    "##Recall/Sensitivity=TP/(TP+FN)\n",
    "##recall how many positives were correctly predicted out total positive..out of the total actual positive model is predicting \n",
    "##how much right positive\n",
    "##Precision...Precision=TP/TP+FP....measure of among all the positive predictions,how many of them were actually positive\n",
    "##For classification problem we don't always go for accuracy so we consider F1_score,recall and precision\n",
    "##becoz sometimes accuracy is high but precision and recall is low\n",
    "##accuracy is not going to give actual confidence of model\n",
    "##accuracy may because of negative value\n",
    "##out of recall n precision...which one to select..so use F1_score\n",
    "##with the help of recall n precision we will be able to predict F1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##A trade off\n",
    "## if recall increases..precision decrease and vice-versa\n",
    "##F1_score is defined as the harmonic mean of precision and Recall\n",
    "##F1_score=2*(Precision+Recall)/(Precision+Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##matrix=confusion_matrix(pred,ytest)\n",
    "##TP=matrix[0][0]\n",
    "##FP=matrix[0][1]\n",
    "##FN=matrix[1][0]\n",
    "##TN=matrix[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##precision=TP/TP+FP\n",
    "##recall=TP/TP+FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##F1_score=2*(recall+precision)/(recall+precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##auc=roc_auc_score(pred,ytest)\n",
    "##auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fpr,tpr,threshold=roc_cureve(pred,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##suppose we built 3 models....1 logistic regression,1 decison tree,1 random forest\n",
    "##they are providing different F1_score\n",
    "##f1 score is..metric that considers both Precision and Recall for evaluating a model\n",
    "##Precision is..amongst all the positive predictions, how many of them were actually positive.\n",
    "##Recall is...from the total number of positive results how many positives were correctly predicted by the model.\n",
    "##..lr giving F1_score..85....dtree giving giving F1_score 84...random forest...F1_score..88\n",
    "##so which model to select....so take help from roc_auc_score\n",
    "##in roc_auc_curve between tpr and fpr..if we plot tpr and fpr..so they both provide roc_curve\n",
    "##auc is area under curve...so roc_auc_score provide which curve covering most of the area\n",
    "##so we pick model with best roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.plot(fpr,tpr,colors=\"orange\",label=\"ROC\")\n",
    "##plt.plot([0,1],[0,1],color=\"darkblue\",linestyle=\"--\",label=\"Roc curve(area= %0.3f)\"% auc)\n",
    "##plt.xlabel(\"false positive rate\")\n",
    "##plt.ylabel(\"true positive rate\")\n",
    "##plt.title(\"Receiver operating characterstics (ROC) curve\")\n",
    "##plt.legend()\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hyperparameter tuning_grid_randomizedsearchcv\n",
    "##Griddearchcv is method to tuen our hyperparameters.We can pass different value of hyperparameters as parameters for grid search.\n",
    "##It does a exhaustive generation of combination of different parameters passed.Using cross validation score,grid search returns\n",
    "## the combination of hyperparameters for which the model is performing the best.\n",
    "##parameter tuning,, It is a way to improve the model accuracy...It helps to optimize the model performance\n",
    "##With best parameters, model will identify the patterns within the dataset in much better way\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "##we are tuning three hyperparameters right now,we are passing the different values of both parameters\n",
    "##grid_param={\"criterion\":[\"gini\",\"entropy\"],\n",
    "##             \"max_depth\":range(2,20,2),\n",
    "##             \"min_samples_leaf\":range(1,10,2),\n",
    "##             \"min_samples_split\":range(2,10,2),\n",
    "##             \"splitter\":[\"best\",\"random\"]\n",
    "##            }\n",
    "\n",
    "##minimum sample leaf..find the best minimum sample leaf\n",
    "##minimum_sample split...select the best value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dtc=DecisionTreeClassifier\n",
    "##grid=GridSearchCV(estimator=dtc,\n",
    "##                  param_grid=grid_param,\n",
    "##                  cv=5\n",
    "##                  n_jobs=-1\n",
    "##                  )\n",
    "\n",
    "##n_jobs=-1 mean i am saying use all available resources to the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##grid.fit(xtrain,ytrain)\n",
    "##best_score=grid.best_score_\n",
    "##best_parameters=grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now use the parameter with classifier\n",
    "##dtc=DecisionTreeClassifier(criterion=[entropy],max_depth=24,min_samples_leaf=1,min_samples_split=2,splitter=\"best\")\n",
    "##dtc.fit(xtrain,ytrain)\n",
    "##dtc.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RandomizedSearchCV\n",
    "##grid_param={\"criterion\":[\"gini\",\"entropy\"],\n",
    "##             \"max_depth\":range(2,20,2),\n",
    "##             \"min_samples_leaf\":range(1,10,2),\n",
    "##             \"min_samples_split\":range(2,10,2),\n",
    "##             \"splitter\":[\"best\",\"random\"]\n",
    "##            }\n",
    "##dtc=DecisionTreeClassifier\n",
    "##rgrid=RandomizedSearchCV(estimator=dtc,\n",
    "##                  param_distribition=grid_param,\n",
    "##                  cv=5\n",
    "##                  n_jobs=-1\n",
    "##                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rgrid.fit(xtrain,ytrain)\n",
    "##best_score=rgrid.best_score_\n",
    "##best_parameters=rgrid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####now use the parameter with classifier\n",
    "##dtc=DecisionTreeClassifier(criterion=[\"entropy\"],max_depth=24,min_samples_leaf=1,min_samples_split=2,splitter=\"best\")\n",
    "##dtc.fit(xtrain,ytrain)\n",
    "##dtc.score(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##randomizedsearchcv..will not produce all the combination of parameters like gridsearchcv.randomizedsearchcv wiill produce\n",
    "##random combination of parameters so it may miss the right most accurate combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PG Program in Data Science, Machine Learning and Neural Networks \n",
    "##Cross validation\n",
    "##Various cross validation technique\n",
    "##to avoid overfitting of model ..we can use cross validation technique\n",
    "##Cross validation is a resampling technique with a basic idea aof dividing the data in 2 parts train and test\n",
    "##using one part we train the model and on the second part data is unseen for the model.we make the prediction and check how well\n",
    "##model works on it.if the model works with good accuracy on your test data.it means model gas not overfitted the training data\n",
    "##and can be trusted with the prediction,whereas if model performs with bad accuracy then model can not be trusted and we need \n",
    "#tweak the algorithm\n",
    "\n",
    "##different approches of cross validation\n",
    "\n",
    "##Hold out Method.....it is the most basic of cv technique in which dataset is divided into 2 sets..traning and test.\n",
    "##training dataset is used to train the model and then test dataset is fitted in the training model to make the predictions\n",
    "##we check the accuracy and assess our model on that basis.This method is used as it is computationally less costly.\n",
    "##But the evaluation based on the hold out set can have a high variance because it depends heavily on which data points end up\n",
    "##in the training set and which in test data.The evaluation is different every time this division changes.\n",
    "\n",
    "##K-fold cross validation\n",
    "##we use k fold cross validation when we have some kind of overfitting problem\n",
    "##we give entire dataset for training and testing by dividing the entire dataset into k-fold subset\n",
    "##suppose k fold is 5 then 5 iteration\n",
    "##we will take average of all the iterations\n",
    "##my data is changes for each iteration and model will not be biased as it has used all the data for training\n",
    "##here cost of training is high as compered to hold out method\n",
    "##cv=i/k summation of i= 1 to k for MSE\n",
    "##to handle the high variance of hold out method ..k fold is used..\n",
    "##idea is simple..divide the whole dataset into k sets preferably of equal size then the firsts et is selected as test set and\n",
    "##the remaining k-1 set are used as tarining data.Again error is calculated.similarly process continues for k times\n",
    "##in the end , the CV error is given as the mean of the total errors calcualted individually\n",
    "##the variance in the error reduces with the increase in k\n",
    "##one disadvantage is that k fold is computationally expensive as algorithm runs from scratch for k times\n",
    "\n",
    "##Leave one out cross validation(LOOCV)\n",
    "##suppose we have 100obseravtion then first one will be treated as test data and 99 will treated as traning data\n",
    "##now this will be repeated 100 times ..in 2nd iteration 2nd observation will be tested and other 99 will be trained\n",
    "##likewise total iteration will be 100\n",
    "##it is one of the costiest approach and computationally expensive as algorithm runs from scratch depending on no of obseravtion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##identifying and treating imbalanced dataset\n",
    "##suppose we test 1000 people and hardly any case turn out to be positive\n",
    "##if we use this for a model..so most of my data will have negative label\n",
    "##if we craete a model using this then model after training will give negative output\n",
    "##because it is train with such data where positive data is not present\n",
    "\n",
    "##in that I have a problem because i will not be able to detect positive output\n",
    "##such data is imbalanced dataset..label are not balanced\n",
    "##imbalaced dataset will be only for classification problem\n",
    "##if ratio is very very low\n",
    "\n",
    "##how to balance the dataset\n",
    "##we want postive n negative same ratio\n",
    "##we have upsampling or oversampling method and downsampling or undersampling method for resolving the imbalanced dataset\n",
    "##import pandas as pd\n",
    "##df=pd.read_csv(\"diabetes.csv\")\n",
    "##df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##df[\"outcomes\"].va;ue_counts()\n",
    "##x=df.drop(columns=[\"outcomes\"])\n",
    "##y=df[\"columns\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from  sklearn.model_selection import test_train_split\n",
    "##xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3)\n",
    "##can also use train_size=0.7 instead pf test_size=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##upsampling\n",
    "##from sklearn.utils import resample\n",
    "##concatenate our training data back together\n",
    "##x=pd.concat([xtrain,ytrain],axis=1)\n",
    "##not_dia=x[x.outcome==0]\n",
    "##diabetic=x[x.outcome==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##dia_upsampled=resample(diabetic,replace=True,n_samples=len(not_dia),random_state=27)\n",
    "##sample=True means sample with replacment\n",
    "##n_samples=len(not_dia) means match number in majority class\n",
    "##random_state=27)...reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##upsampled=pd.concat([not_dia,dia_upsampled])\n",
    "##upsampled.outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Dowmsampling\n",
    "##not_dia_sampled=resample(not_dia,replace=False,n_samples=len(diabetic,random_state=27))\n",
    "##downsampled=pd.concat([not_dia_sampeld,diabetic])\n",
    "##doensampled.outcome.value_counts()\n",
    "##here we are downsampling so replacement=False...here majority to minority(downsampling)\n",
    "##minority to majority(upsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##when to downsample and when to upsample totally depend on data\n",
    "##if we are having hugh record then millions of record then if data unbalanced then downsample\n",
    "##if having small datadets in lakhs then upsample in case data is unbalanced\n",
    "##downsampling..Decreasing the sample size of higher class\n",
    "##upsampling...Increasing the sample size of lower class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PCA\n",
    "##dimensionality reduction tecnique\n",
    "##prerequisite for PCA miu(mean)=0 and standard deviation=1\n",
    "##we convert no of features into no of componenets\n",
    "##so as we plot  x and y we find mean is not zero\n",
    "##so we try to make miu equals to zero by data shifting..this shifting called parallel translation\n",
    "##so then we find best best line covering maximum data points with small error\n",
    "##eigen values and eigen vector\n",
    "##this line will move in the direction where maximum data pints..this data pints are eigen vectors\n",
    "##eigen value..we will use eigen value for model building\n",
    "##eigen value...\n",
    "##scree plot...how many componenst we will use..\n",
    "##when can we apply PCA..if we have 200 or 1000 feature then we can apply PCA\n",
    "##as feature increse so we will have issue with visualization and also computationally expensive\n",
    "##in order to overcome curse of dimensionality we will use PCA\n",
    "##ss=StandardScaler()\n",
    "##x_trans=ss.fit_transform(x)\n",
    "##from sklearn.decomposition import PCA\n",
    "##pca=PCA()\n",
    "##xcomp=pca.fit_transform(x_trans)\n",
    "##plt.figure()\n",
    "##plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "##plt.xlabel(\"no of components\")\n",
    "##plt.ylabel(\"variance\")\n",
    "##plt.title(\"expaliend variance\")\n",
    "##plt.show()\n",
    "##if 95%  of the variance is explained at component 8..so instead of going for all 11 columns ..let us use these 8 componenet\n",
    "\n",
    "##pca=PCA(n_components=8)\n",
    "##xnew=pca.fit_transform(x_trans)\n",
    "##xprinp=pd.DataFrame(xnew,columns=[\"Pc1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##xprinp\n",
    "##xtrain,xtest,ytrain,ytest=train_test_split(xprinp,y,test_size=.30,random_state=355)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SVM..Support vector machine\n",
    "##supervised amchine algorithm\n",
    "##we can use SVM for regression and classification...sVR n SVC (support vector regression n support vector classification)\n",
    "##Given the datastet algorithm tries to divide the data using hyperplanes and then makes the prediction\n",
    "##SVM is non probabilistic linear classifier\n",
    "##while other classifier while classifying predict the proability of the datapoint to belong to one group or the another.\n",
    "##SVM directly says to which group datapoint belongs to without using any proability calculation\n",
    "##KNN probabilistic classifier\n",
    "##which way to divide in a way that the boundary point of two group have maximum distance\n",
    "##we divide using hyperplane and boundary points are support vector\n",
    "##distance between two set of data is maximum margin\n",
    "##the best line to create maximum margin is optimum classifier\n",
    "##kernel methods comes into picture to separate the datapoints using some mathimatical function if data is not linearly separable\n",
    "##in 2d hyperplane is a line\n",
    "##hyperplane is n-1 dimensional plane which optimally divides the data of n dimensions\n",
    "##in 2-D support vectors will be point and in multi-dimensionalal space they will eb vectors\n",
    "##hence the name support vector machine as the algorithm creates the optimum classification line by maximising its distance from\n",
    "##the two support vector\n",
    "##when the data is not linearly separable then to create a hyperplane to separate the data into diffrent groups,SVM need to perform \n",
    "##computation in a higher dimensional space\n",
    "##But the introduction of new dimension make the computation for the SVM more intensive which impacts the algorithm performance\n",
    "##To rectify this mathematicians came up with the approach of K methods.\n",
    "##K methods use the kernel functions available in mathematics.\n",
    "##the unique feature of kernel function is to compute in the higher dimensional space without calculating the new coordinates in \n",
    "##that higher dimension.It implicitly uses predefined mathematical fucntions to do operations on the existing points which mimic\n",
    "## the computation in the higher dimensional space without adding the computation cost as they are not actually calculating the \n",
    "##coordinates in the higher dimension thereby avoiding the computation of calculating distances from the newly computed points.\n",
    "##This is called the kernel trick\n",
    "##suppose we have a non linear distribution of data as we cannot classify the data using the linear equation.\n",
    "##so we project the data in a 3-dimensional space and derives the plane which divides the data in 2 parts.in theory that is what\n",
    "##a kernel function does without computing the additional coordiantes for the higher diemnsion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##K-nearest neighbours(KNN)\n",
    "##we want o classify data..so we use the neighbours\n",
    "##here we need to give k(neighbours) suppose 3\n",
    "##it will calculate the distance of a point from all the points and cosider the 3 closest points as k=3\n",
    "##so suppose out of 3 2 is pink and 1 is blue is proability is 2/3.we will classify data on basis of higher probability of the \n",
    "##nearest ponits..if proability of pink is high so classify as pink\n",
    "##if k=5..if out 5 neasrest neighbour 3 is blue n 2 is pink then classify pint as blue\n",
    "##dpending upon k class might change\n",
    "##k-NN algorithm.... It can be used in both classification and regression\n",
    "\n",
    "##tehcniques..we use in KNN\n",
    "##Euclidean distance\n",
    "##Manhattan distance\n",
    "##Hamming distance\n",
    "\n",
    "##popularly we use euvlidian distance as it give better result\n",
    "##euclidian distance=squareroot of (p2-p1)**2+(q2-q1)**2\n",
    "\n",
    "##manhatten distance\n",
    "\n",
    "##we also call KNN as lazy learner..normally ew train the model and it is ready then we apply test data\n",
    "##but in KNN modle in not going to train until we pass test data..as soon as the test data is passed it start training\n",
    "##until and unless we don;t have test data to predict it will not train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##KNN Model Building\n",
    "##import pandas as pd\n",
    "##import numpy as np\n",
    "##import matplotlib.pyplot as plt\n",
    "##import seaborn as sns\n",
    "##from model_selection import train_test_split,GridSearchCV\n",
    "##from model_selection import KFold\n",
    "##from sklearn.neighors import KNeighborsClassifier\n",
    "##from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,roc_auc_score\n",
    "##from sklearn.preprocessing import StandardScaler\n",
    "##from statsmodesl.stats.outliers_influence import variance_inflation_factor\n",
    "##sns.set()\n",
    "##import warnings\n",
    "##warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data=pd.read_csv(\"diabetes,csv\")\n",
    "##data.head()\n",
    "##data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let see the distribution of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.figure(figsize=(20,25),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##for column in data:\n",
    "##    if plotnumber<=9:\n",
    "##        ax=plt.subplot(3,3,plotnumber)\n",
    "##        sns.distplot(data[column])\n",
    "##        plt.xlabel(column,fontsize=20)\n",
    "##    plotnumer+=1\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##some feature has zero value which is not possible in realworld scenario\n",
    "##and in some feature we can look skewness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fig,ax=plt.subplots(figsize(15,10))\n",
    "##sns.boxplot(data=data,width=0.5,ax=ax,fliersize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##to remove skewness\n",
    "##q=data[\"pregnancies\"].quantile(0.98)\n",
    "##data_cleaned=data[data[\"pregnancies\"]<q]\n",
    "\n",
    "##q=data_cleaned[\"BMI\"].quantile(0.99)\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"BMI\"]<q]\n",
    "\n",
    "##q=data_cleaned[\"skin thickness\"].quantile(0.99)\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"thickness\"]<q]\n",
    "\n",
    "##q=data_cleaned[\"insulin\"].quantile(0.95)\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"insulin\"]<q]\n",
    "\n",
    "##q=data_cleaned[\"diabetespedigreefunction\"].quantile(0.99)\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"diabetespedigreefunction\"]<q]\n",
    "\n",
    "##q=data_cleaned[\"age\"].quantile(0.98)\n",
    "##data_cleaned=data_cleaned[data_cleaned[\"age\"]<q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plt.figure(figsize=(20,25),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##for column in data:\n",
    "##    if plotnumber<=9:\n",
    "##        ax=plt.subplot(3,3,plotnumber)\n",
    "##        sns.distplot(data[column])\n",
    "##        plt.xlabel(column,fontsize=20)\n",
    "##    plotnumer+=1\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##x=data.drop(columns=[outcome])\n",
    "##y=data[\"outcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####plt.figure(figsize=(20,25),facecolor=\"white\")\n",
    "##plotnumber=1\n",
    "##for column in data:\n",
    "##    if plotnumber<=9:\n",
    "##        ax=plt.subplot(3,3,plotnumber)\n",
    "##        sns.stripploty,x[column])\n",
    "##    plotnumer+=1\n",
    "##plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ss=StandardScaler()\n",
    "##xscaled=ss.fit_ytransform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##vif\n",
    "##vif=pd.DataFrame()\n",
    "##vif[\"vif\"]=[variance_inflation_factor(xscaled,i for i in range(xscaled.shape[1])]\n",
    "##vif[\"features\"]=x.columns\n",
    "##vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##xtrain,xtest,ytrain,ytest=train_test_split(x_scaled,y,test_size=0,25,random_state=27)\n",
    "##knn=KNeighborCLassifier()\n",
    "##knn.fit(xtrain,ytrain)\n",
    "##knn_pred=knn.predict(xtest)\n",
    "##knn_score(x_train,y_train)\n",
    "##print(\"accuracy_score\",accuracy_score(ytest,knn_pred))\n",
    "##print(\"classification report\",classification_report(ytest,knn_pred))\n",
    "##print(\"confusion_matrix\",confusion_matrix(ytest,knn_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##grid={\"algorithm\":[\"ball_tree\",\"kd_tree\",\"brute\"],\n",
    "##                  \"leaf_size\":[18,20,25,27.30.32,34],\n",
    "##                  \"n_neighbors\":[3,5,7,9,10,11,12,13]}\n",
    "##girdsearch=GridSearchCV(knn,param_grid,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##gridsearch.fit(x_train,y_train)\n",
    "##gridsearch.best_score_\n",
    "##gridsearch.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we got best parameters now\n",
    "##knn=KNeighborsClassifier(algorithm=ball_tree,leaf_size=18,n_neighbors=11)\n",
    "##knn.fit(xtrain.ytrain)\n",
    "##knn.score(xtrain.ytrain)\n",
    "##knn.predict(xtest)\n",
    "##knn.score(xtest,ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The boundary becomes smoother with increasing value of K\n",
    "## k-NN does not require an explicit training step\n",
    "## We can choose optimal value of k with the help of cross validation\n",
    "## Euclidean distance treats each feature as equally important\n",
    "\n",
    "##our accuracy score has increased for our test data,so our model was overfitting before\n",
    "##now use k-fold cross validation and check how well our model is generalizing over our dataset.we are randomly selecting\n",
    "##our k to be 12 for k fold\n",
    "##k-fold cross validation\n",
    "##kfold=kFold(n_splits=12,random_state=42)\n",
    "##kfold.get_n_splits(xscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##from statistics import mean\n",
    "##knn=KneighborsClassifier(algorithm=ball_tree,leaf_size=18,n_neighbors=11)\n",
    "##cnt=0\n",
    "##count=[]\n",
    "##train_score=[]\n",
    "##test_score=[]\n",
    "##for train_index,test_index in kfold.split(xscaled):\n",
    "##    xtrain,xtest=xscaled[train_index],xscaled[test_index]\n",
    "##    ytrain.ytest=y.iloc[train_index],y.iloc[test_index]\n",
    "##    knn.fit(xtrain,ytrain)\n",
    "##    train_score=knn.score(xtrain,ytrain)\n",
    "##    test_score=knn.score(xtest,ytest)\n",
    "##    cnt=cnt+1\n",
    "##    count=append(cnt)\n",
    "##    train_score.append(train_score_)\n",
    "##    test_score.append(test_score_)\n",
    "##    print(\"for k =\",cnt)\n",
    "##    print(\"train score is\",train_score_,\"and test score is\",test_score_)\n",
    "\n",
    "##print(\"average train score is\",mean(train_score))\n",
    "##print(\"average test score is\",mean(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now plot the outcome\n",
    "##plt.plot(count,test_score)\n",
    "##plt.xlabel(\"value of k for k fold\")\n",
    "##plt.ylabel(\"test accuracy\")\n",
    "##plt.xticks(np.arange(0,12,1))\n",
    "##plt.yticks(np.arange(0.65,1,.05))\n",
    "## thats how we can pick k fold"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
