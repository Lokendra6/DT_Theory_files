{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##projects.datatrained.com...for portal\n",
    "##practise project and evaluation project\n",
    "##as well as deadlines\n",
    "##Standard Scaler is for scaling the datapoints in the range of +3 to -3, considering some error. It won't remove outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PCA example..to reduce the dimensionality reduction...column * rows (1000col*100row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##95% inpformation must be retained while using PCA....PCA to used if 200 or 300 column \n",
    "##import numpy as np\n",
    "##import pandas as pd\n",
    "##import matplotlib.pyplot as plt\n",
    "##before PCA use standard scaler\n",
    "##and after preprocessing use tsandard scaler\n",
    "##df=pd.read_csv()\n",
    "##df=pd.read_csv(\"TestPCA.csv\")\n",
    "##df.head()\n",
    "##df.dropna()..demo so directly dropping otherwise follow peocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fro sklearn.decomposition import PCA\n",
    "##ss=StandardScaler()\n",
    "##scaledx=ss.fit_transform(df)\n",
    "##\n",
    "##testPCA=PCA()\n",
    "##y=testPCA.fit_transform(scaledX)\n",
    "##here we didn't mentioned the no of components why becoz\n",
    "\n",
    "##cumsum is cumulative sum\n",
    "##explained variance ratio..variance for each componentsor variable or columns\n",
    "##y.expalained_vriance_ratio\n",
    "##var_cum=np.cumsum(y.explained_variance_ratio_)*100\n",
    "\n",
    "##so at column..on 31 column 90 percent variance is explained \n",
    "##generally we take 90%\n",
    "##so we can reduce the number of column to 31 and retain 90% of information\n",
    "##plt.xlabel(\"principal components\")\n",
    "##plt.ylabel(\"cumulative explained variance\")\n",
    "##plt.axvline(x=k,color=\"k\",linestyle=\"--\")\n",
    "##plt.axhline(y=90,color=\"r\",linestyle=\"--\")\n",
    "##plt.plot(var_cum)\n",
    "##plt.show()\n",
    "##we can get the no of componenents expalining how much variance or information through plot alsoS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get optimum number automatically calculated for hugh data\n",
    "##how many PCS explain 95% of the variance\n",
    "##k=np.argmax(var_cum>95)\n",
    "##print(\"no of components explaining  95% variance\",k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get optimum number automatically calculated for hugh data\n",
    "##how many PCS explain 95% of the variance\n",
    "##k=np.argmax(var_cum>90)\n",
    "##print(\"no of components explaining  95% variance\",k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##now pass the number of componenets as 30 for 90% and 35 for 95% in PCA toreduce the dimensionality\n",
    "##FinalPCA=PCA(n_components=30)\n",
    "##FinalData=FinalPCA.fit_transform(Xscaled)\n",
    "##finaldata=pd.DataFrame(FinalData)\n",
    "\n",
    "##Standard Scaler and PCA is applied only on the features column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature Extraction\n",
    "\n",
    "##import pandas as pd\n",
    "##df=pd.read_csv(\"attendance.csv\")\n",
    "##df.shape\n",
    "##df.dtypes\n",
    "##df.isnul().sum()\n",
    "##check for null values..then check for datatype\n",
    "##if a dataset is in data column then how to extract feature in that case\n",
    "##first convert date which is in object type to date type\n",
    "##df[\"date\"]=pd.to_datetime(df[\"date\"])\n",
    "##df.datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we will create a new column and extract the year\n",
    "##we will create a new column and extract the month\n",
    "##from existing colum date we are craeting 2 columns\n",
    "##df[\"year\"]=df[\"date\"].dt.year\n",
    "##df[\"month\"]=df[\"month\"].dt.month\n",
    "##now we can drop date\n",
    "##df.drop(\"date\",axis=1,inplace=True)\n",
    "\n",
    "##for day we use...df[\"day\"]=df[\"date\"].dt.day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
